<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Past Projects on Beiming&#39;s World</title>
        <link>https://ocarinaz.github.io/categories/past-projects/</link>
        <description>Recent content in Past Projects on Beiming&#39;s World</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Tue, 07 Dec 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://ocarinaz.github.io/categories/past-projects/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Portfolio Construction Constraints and Portfolio Volatility Range</title>
        <link>https://ocarinaz.github.io/p/orie5630/</link>
        <pubDate>Tue, 07 Dec 2021 00:00:00 +0000</pubDate>
        
        <guid>https://ocarinaz.github.io/p/orie5630/</guid>
        <description>&lt;img src="https://ocarinaz.github.io/p/orie5630/Cover.jpg" alt="Featured image of post Portfolio Construction Constraints and Portfolio Volatility Range" /&gt;&lt;p&gt;The project is based on the box constraint of portfolio efficient frontier construction with different constraint methods used on simulated resampling data to find the relationship between constraints and minimum variance or tangent portfolio volatility range calculated from the resamples. The main methodologies are multivariate t-distribution fitting and bootstrapping based on estimated parameters. Data source is the daily return on 9 stocks in recent 15 years.&lt;/p&gt;
&lt;p&gt;Main findings are as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Ridge constraint is tighter than box constraint in efficient frontier value range. The minimum variance it can reach is larger than that of box constraint situation.&lt;/li&gt;
&lt;li&gt;For minimum variance portfolios, box constraints have smaller value and range than ridge constraint. That is, when using box constraints on the sample to calculate minimum variance portfolio, it is more likely to be a more precise estimation of true minimum variance.&lt;/li&gt;
&lt;li&gt;For tangent portfolios, ridge constraint reaches the minimum volatility and range. Also, tangent portfolios are more largely influenced by the change of constraints.&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>Spoken Digit-Pair Recognition</title>
        <link>https://ocarinaz.github.io/p/ece5420/</link>
        <pubDate>Mon, 06 Dec 2021 00:00:00 +0000</pubDate>
        
        <guid>https://ocarinaz.github.io/p/ece5420/</guid>
        <description>&lt;img src="https://ocarinaz.github.io/p/ece5420/Cover.jpeg" alt="Featured image of post Spoken Digit-Pair Recognition" /&gt;&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;In the project, mainly 4 kinds of machine learning methods (Naïve Bayesian, Logistic Regression, Support Vector Machine and Decision Tree) are tested, along with the additional processing with the ‘missing’ label ’43’ in training set. In addition, time efficiency of all methods is also recorded. As a result, the Logistic Regression has the highest prediction accuracy (92.6%) with relatively short time consumed.&lt;/p&gt;
&lt;h2 id=&#34;work-details&#34;&gt;Work Details&lt;/h2&gt;
&lt;p&gt;As mentioned in the overview, the most challenging part of dealing with the spoken digit-pair data set is that label ‘43’ is not included in the training set, but it appears in the testing set. Thus, firstly all four ML methods are tried on the training data to find the ones that have lowest estimation error, and then different kinds of algorithms are used to figure out the ‘43’ label in testing data set based on the selected ML method.&lt;/p&gt;
&lt;p&gt;As for implementing ML methods on training data, considering about the space it needs to take for reading and analyzing all 90000 training data, the project used batching to save space while maintaining the effect of training on large data set. That is: (1) randomly select 10000 data from the whole training data set each time and separate it into training and testing set; (2) train/update the machine learning model on this new smaller data set; (3) repeat (1) and (2) and update the original machine learning model in each batch (9 batches in all). As a result, the logistic regression and SVC model achieved the highest accuracy (about 91%). A bagging-like process that selected the most commonly appearing labels in 18 batches (5000 data in each loop) for each testing data was also tried, but its result was not as good as not doing extra manipulation.&lt;/p&gt;
&lt;p&gt;Based on the basic ML model selected above, when it comes to dealing with label ‘43’ that doesn’t appear in the training set, 3 algorithms were tried:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Algorithm 1: randomly swap the order of the two labels of one data in order to make the combination of ‘43’ possible, split the labels of all training data (after the swap) into two label series, and train the machine learning model on each series separately. Finally, combine the single label predictions from two models into the series of two labels for one testing data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Algorithm 2: make a copy of training data, split the 2-digit label into two 1-digit labels for each training data and distribute the 1-digit labels to training data (e.g. (X,’32’) -&amp;gt; (X,’3’), (X.’2’)). Then train the machine learning model on this new training set and select the top 2 labels with the largest probability.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Algorithm 3: as ‘43’ doesn’t exist in training set, it might be possible to find a proper threshold on the probability so that we can distinguish the data in testing set whose true label is ‘43’ but mistakenly classified as other labels.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a result, Algorithm 1 would produce too many labels with the same number such as ‘11’ or ‘22’, which may be explained by the fact that when trained separately, some of the data will be treated as noise, and with only one label given to actually two digit’s wave data, true data itself may be treated as noise and thus leads to the repeated label in 2-digit labels.
As a solution to this problem, Algorithm 2 is designed. This algorithm solves both the problem of missing ‘43’ and misrecognizing noise by including both labels separately in the training set. Logistic regression based on this algorithm reaches the accuracy of 92.6%.
Algorithm 3 seems to work but actually it is difficult to find a threshold. The testing data with true label ‘43’ still have quite high prediction probability on other labels (e.g. 90% for label ‘41’ when true label is ‘43’).&lt;/p&gt;
&lt;p&gt;Finally, Algorithm 2 is chosen, and further research about other more complicated models such as HMM or CNN to deal with the ‘unobservable’ recognizing process and figure out ‘43’.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Stock Price Prediction Based On Hidden Markov Model</title>
        <link>https://ocarinaz.github.io/p/diploma1st/</link>
        <pubDate>Mon, 21 Jun 2021 00:00:00 +0000</pubDate>
        
        <guid>https://ocarinaz.github.io/p/diploma1st/</guid>
        <description>&lt;img src="https://ocarinaz.github.io/p/diploma1st/Cover.jpg" alt="Featured image of post Stock Price Prediction Based On Hidden Markov Model" /&gt;&lt;p&gt;Recently, Hidden Markov Model (HMM) has achieved certain application and development in stock market price and trend prediction. Existing HMM-based price prediction methods mainly predict the closing price of the next day by either (1) likelihood: finding price sequences in historical data which have similar pattern with current data; (2) MAP: discretizing the range of observable variable to form thousands of alternatives, from which the one with highest MAP will be selected as price prediction. The two approaches above face the problems of excessive computational costs or lacking double-check of posterior probability, where still remains possibility of optimization. Thus, this article put forward two new methods of price prediction, namely, likelihood-MAP approach and likelihood-posterior-probability-weighted approach, based on the analysis of traditional likelihood and MAP approaches, aiming at improving prediction accuracy as well as reducing computational burden in time. These two new methods correspond to two different optimization approaches, namely (1) completing a MAP double-check using alternatives (normally only dozens) drawn from likelihood method to obtain the prediction (likelihood-MAP approach); (2) using posterior probability as the weight of alternatives to get a weighted price as prediction.&lt;/p&gt;
&lt;p&gt;As for empirical aspect, the research has used CSI 300 Index as data source to construct 105 technical factor groups as the observable variables of HMM, with different window length and different prediction length have also been taken into consideration as robustness test of the new prediction approaches. The empirical results show that the new approaches in this article have better performances than traditional ones, which is in accordance with the expectations of this research, and may be able to provide references for further trading strategy construction.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Fandom Identity Perception In “Self-made Merch For Free” Activity</title>
        <link>https://ocarinaz.github.io/p/diploma2nd/</link>
        <pubDate>Mon, 14 Jun 2021 00:00:00 +0000</pubDate>
        
        <guid>https://ocarinaz.github.io/p/diploma2nd/</guid>
        <description>&lt;img src="https://ocarinaz.github.io/p/diploma2nd/Theory.jpeg" alt="Featured image of post Fandom Identity Perception In “Self-made Merch For Free” Activity" /&gt;&lt;p&gt;As a type of offline fandom activity, “Self-made Merch For Free” has been an emerging communication activity among stage actor/actress fans. Different from traditional supporting activities or club meetings, this activity is characterized by its non-profit, absence of the star himself/herself, non-promoting, and weak relationships between participants, but still, it has certain influence on the formation of identity of the fans. However, existing researches mainly focus on online communication when analyzing fandom identity, rather than those happening in offline physical space. Thus, based on interaction ritual theory as analyzing frame, this thesis has explored the construction process and causes of perception of identity during “Self-made Merch For Free” activity conducted by stage actor fandom through field research, online participatory observation, semi-structured in-depth interviews and questionnaire survey. Its influences and significance are also summarized and discussed.&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
